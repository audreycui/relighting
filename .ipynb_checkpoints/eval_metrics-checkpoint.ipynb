{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lpips\n",
    "import os\n",
    "from PIL import Image \n",
    "import torch \n",
    "from rewrite_utils import renormalize, show\n",
    "import glob \n",
    "\n",
    "lpips_metric = lpips.LPIPS(net='alex')\n",
    "mse_metric = torch.nn.MSELoss()\n",
    "\n",
    "#local mean squared error from intrinsic image algorithms\n",
    "import numpy as np\n",
    "\n",
    "def local_error(correct, estimate, mask, window_size, window_shift):\n",
    "    \"\"\"Returns the sum of the local sum-squared-errors, where the estimate may\n",
    "    be rescaled within each local region to minimize the error. The windows are\n",
    "    window_size x window_size, and they are spaced by window_shift.\"\"\"\n",
    "    M, N = correct.shape[1:]\n",
    "    ssq = total = 0.\n",
    "    for i in range(0, M - window_size + 1, window_shift):\n",
    "        for j in range(0, N - window_size + 1, window_shift):\n",
    "            correct_curr = correct[:, i:i+window_size, j:j+window_size]\n",
    "            estimate_curr = estimate[:, i:i+window_size, j:j+window_size]\n",
    "            mask_curr = mask[:, i:i+window_size, j:j+window_size]\n",
    "            ssq += ssq_error(correct_curr, estimate_curr, mask_curr)\n",
    "        total += np.sum(mask_curr * correct_curr**2)\n",
    "    assert ~np.isnan(ssq/total)\n",
    "    return ssq / total\n",
    "\n",
    "def ssq_error(correct, estimate, mask):\n",
    "    \"\"\"Compute the sum-squared-error for an image, where the estimate is\n",
    "    multiplied by a scalar which minimizes the error. Sums over all pixels\n",
    "    where mask is True. If the inputs are color, each color channel can be\n",
    "    rescaled independently.\"\"\"\n",
    "    assert correct.ndim == 3\n",
    "    if np.sum(estimate**2 * mask) > 1e-5:\n",
    "        alpha = np.sum(correct * estimate * mask) / np.sum(estimate**2 * mask)\n",
    "    else:\n",
    "        alpha = 0.\n",
    "    return np.sum(mask * (correct - alpha*estimate) ** 2)\n",
    "\n",
    "def pilim(img): \n",
    "    return renormalize.from_image(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"evaldata/ada_lonoff\"\n",
    "truth_path = \"datasets/lonoff_light_all/test_B/\"\n",
    "\n",
    "lpips_all = []\n",
    "mse_all = []\n",
    "lmse_all = []\n",
    "\n",
    "def pilim(img): \n",
    "    return renormalize.from_image(img)\n",
    "\n",
    "i = 0\n",
    "for img_path in os.listdir(truth_path): \n",
    "    i+=1\n",
    "    print(f'{i} {img_path}')\n",
    "    w, h = (256, 256)\n",
    "    truth = pilim(Image.open(os.path.join(truth_path, img_path)).resize((h, w), Image.BILINEAR))\n",
    "    img_path = img_path[4:-4]\n",
    "    \n",
    "    lpips_temp = []\n",
    "    mse_temp = []\n",
    "    lmse_temp = []\n",
    "    mask = np.ones(truth.shape)\n",
    "    \n",
    "    for path in glob.glob(os.path.join(test_path, f'{img_path}_output_image*')): \n",
    "        test = pilim(Image.open(os.path.join(path)).resize((h, w), Image.BILINEAR))\n",
    "        lpips_temp.append(torch.squeeze(lpips_metric(test, truth)).item())\n",
    "        mse_temp.append(torch.squeeze(mse_metric(test, truth)).item())\n",
    "        lmse_temp.append(local_error(truth.numpy(), test.numpy(), mask, 20, 10))\n",
    "        \n",
    "    lpips_all.append(min(lpips_temp))\n",
    "    #lpips_baseline_all.append(torch.squeeze(lpips_metric(original, truth)))\n",
    "    \n",
    "    mse_all.append(min(mse_temp))\n",
    "    #mse_baseline_all.append(torch.squeeze(mse_metric(original, truth)))\n",
    "    \n",
    "    lmse_all.append(min(lmse_temp))\n",
    "    #lmse_all.append(local_error(truth.numpy(), test.numpy(), mask, 20, 10))\n",
    "     \n",
    "#lpips_baseline_avg = torch.mean(torch.stack(lpips_baseline_all))\n",
    "#mse_basline_avg = torch.mean(torch.stack(mse_baseline_all))  \n",
    "#lpips_avg = torch.mean(torch.stack(lpips_all))\n",
    "#mse_avg = torch.mean(torch.stack(mse_all))\n",
    "lpips_avg = sum(lpips_all)/len(lpips_all)\n",
    "mse_avg = sum(mse_all)/len(mse_all)\n",
    "lmse_avg = sum(lmse_all)/len(lmse_all)\n",
    "print(f'lpips: {lpips_avg}')\n",
    "print(f'mse: {mse_avg}')\n",
    "print(f'lmse: {lmse_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips_avg = sum(lpips_all)/len(lpips_all)\n",
    "mse_avg = sum(mse_all)/len(mse_all)\n",
    "lmse_avg = sum(lmse_all)/len(lmse_all)\n",
    "print(f'lpips: {lpips_avg}')\n",
    "print(f'mse: {mse_avg}')\n",
    "print(f'lmse: {lmse_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(truth.shape)\n",
    "lmse = local_error(truth.numpy(), test.numpy(), mask, 20, 10)\n",
    "print(lmse)\n",
    "lmse = local_error(truth.numpy(), original.numpy(), mask, 20, 10)\n",
    "print(lmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_edges_canny(im): \n",
    "    im = cv2.cvtColor(np.array(im).reshape(256, 256, 3), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    im = cv2.Canny(image=np.uint8(im), threshold1=100, threshold2=200)\n",
    "    im = cv2.convertScaleAbs(im, alpha=255/im.max())\n",
    "    im = torch.tensor(im/255).unsqueeze(0).repeat(3, 1, 1).float() #dimension of (3, 256, 256) needed for lpips\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"results/modulated_bedrooms_9resnet/test_230_generated/images/\"\n",
    "\n",
    "lpips_baseline_all = []\n",
    "mse_baseline_all = []\n",
    "lpips_all = []\n",
    "mse_all = []\n",
    "\n",
    "def pilim(img): \n",
    "    return renormalize.from_image(img)\n",
    "\n",
    "num_imgs = 100\n",
    "for i in range(num_imgs): \n",
    "    print(i)\n",
    "    img_path = f\"bedroom_{i}\"\n",
    "    \n",
    "    stylespace = (Image.open(\n",
    "                os.path.join(test_path, img_path+\"_stylespace_target.jpg\")))\n",
    "    stylespace = get_edges_canny(stylespace)\n",
    "    \n",
    "    test = (Image.open(\n",
    "                os.path.join(test_path, img_path+\"_output_image.jpg\")))\n",
    "    test = get_edges_canny(test)\n",
    "    \n",
    "    original = (Image.open(\n",
    "                os.path.join(test_path, img_path+\"_input_image.jpg\")))\n",
    "    original = get_edges_canny(original)\n",
    "    \n",
    "    if i%10 == 0: \n",
    "        show([(renormalize.as_image(original), renormalize.as_image(test), renormalize.as_image(stylespace))])\n",
    "\n",
    "    lpips_all.append(torch.squeeze(lpips_metric(original, test)))\n",
    "    lpips_baseline_all.append(torch.squeeze(lpips_metric(original, stylespace)))\n",
    "    \n",
    "    mse_all.append(torch.squeeze(mse_metric(original, test)))\n",
    "    mse_baseline_all.append(torch.squeeze(mse_metric(original, stylespace)))\n",
    "     \n",
    "lpips_baseline_avg = torch.mean(torch.stack(lpips_baseline_all))\n",
    "mse_basline_avg = torch.mean(torch.stack(mse_baseline_all))  \n",
    "lpips_avg = torch.mean(torch.stack(lpips_all))\n",
    "mse_avg = torch.mean(torch.stack(mse_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lpips: {lpips_avg}')\n",
    "print(f'mse: {mse_avg}')\n",
    "print(f'lpips baseline: {lpips_baseline_avg}')\n",
    "print(f'mse baseline: {mse_basline_avg}')\n",
    "\n",
    "mask = np.ones(test.shape)\n",
    "lmse = local_error(original.numpy(), test.numpy(), mask, 20, 10)\n",
    "print(lmse)\n",
    "lmse = local_error(original.numpy(), stylespace.numpy(), mask, 20, 10)\n",
    "print(lmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_path = \"results/modulated_bedrooms_9resnet/test_500_real_datasets_lonoff_light_all_linspace_on/images\"\n",
    "ablation_path =\"results/modulated_bedrooms_9resnet/test_500_real_datasets_lonoff_light_all_linspace_on/images\"\n",
    "ada_path = \"evaldata/ada_lonoff\"\n",
    "e4e_path = \"evaldata/e4e_lonoff_sweep\"\n",
    "truth_path  = \"datasets/lonoff_light_all/test_B/\"\n",
    "input_path = \"datasets/lonoff_light_all/test_A\"\n",
    "\n",
    "paths = [alternate_path, ada_path, e4e_path]\n",
    "\n",
    "i = 0\n",
    "for img_path in os.listdir(truth_path):\n",
    "    i+=1\n",
    "    print(f'{i} {img_path}')\n",
    "    w, h = (256, 256)\n",
    "    truth = (Image.open(os.path.join(truth_path, img_path)).resize((h, w), Image.BILINEAR))\n",
    "    img_path = img_path[4:-4]\n",
    "    \n",
    "    imgs = []\n",
    "    imgs.append((Image.open(os.path.join(input_path, f'{img_path}.jpg')).resize((h, w), Image.BILINEAR)))\n",
    "    imgs.append(truth)\n",
    "    for path in paths: \n",
    "        imgs.append(Image.open(os.path.join(path, f'{img_path}_output_image_1.jpg')).resize((h, w), Image.BILINEAR))\n",
    "    \n",
    "    show([tuple(imgs)])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}