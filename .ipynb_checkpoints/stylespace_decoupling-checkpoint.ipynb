{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rewrite_utils import zdataset, show, labwidget, paintwidget, renormalize, nethook, imgviz, pbar, smoothing\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "from rewrite_utils.stylegan2 import load_seq_stylegan\n",
    "import copy, contextlib\n",
    "from rewrite_utils.stylegan2.models import DataBag\n",
    "\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyMaskedStyle(torch.nn.Module):\n",
    "    def __init__(self, mask, frac):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mask', mask)\n",
    "        #self.register_buffer('fixed_style', fixed_style)\n",
    "        self.unit = 265\n",
    "        self.frac = frac\n",
    "        \n",
    "    def forward(self, d):\n",
    "        modulation_light = d.style.detach().clone() \n",
    "        modulation_light[:, self.unit] = self.frac\n",
    "        \n",
    "        modulation_no_light = d.style.detach().clone()        \n",
    "        #modulation_no_light[:, self.unit] = 0\n",
    "        \n",
    "        modulation = (modulation_light[:,:,None,None] * self.mask) + (\n",
    "            modulation_no_light[:,:,None,None] * (1 - self.mask))\n",
    "        return DataBag(d, fmap=modulation * d.fmap,\n",
    "             style=d.style)\n",
    "    \n",
    "def make_masked_stylegan(gan_model, initial_z, mask_url, frac):\n",
    "    '''\n",
    "    Given a stylegan and a mask (encoded as a PNG) and an initial z,\n",
    "    creates a modified stylegan which applies z only to a masked\n",
    "    region.\n",
    "    '''\n",
    "    layer = 'layer8.sconv.mconv.adain'\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        style_layers = [n for n, _ in gan_model.named_modules() if 'adain' in n]\n",
    "        with contextlib.ExitStack() as stack:\n",
    "            retained_inputs = {\n",
    "                layer: stack.enter_context(nethook.Trace(gan_model, layer, retain_input=True))\n",
    "                for layer in style_layers}\n",
    "            gan_model(initial_z)\n",
    "            style_vectors = {layer: retained_inputs[layer].input.style\n",
    "                             for layer in style_layers}\n",
    "            style_shapes = {layer: retained_inputs[layer].output.fmap.shape\n",
    "                             for layer in style_layers}\n",
    "        masked_model = copy.deepcopy(gan_model)\n",
    "        device = next(masked_model.parameters()).device\n",
    "\n",
    "        parent = nethook.get_module(masked_model, layer[:-len('.adain')])\n",
    "        #vec = style_vectors[layer].to(device)\n",
    "        shape = style_shapes[layer][-2:]\n",
    "        downsize = Resize(shape)\n",
    "        \n",
    "        #mask = renormalize.from_url(mask_url, target='pt', size=shape)[0][None, None]\n",
    "        #print(renormalize.from_url(mask_url, target='pt', size=shape).shape)\n",
    "        #print(mask.shape)\n",
    "        mask = mask_url[None, None, :, :]\n",
    "        mask = downsize(mask)\n",
    "        #print(mask.shape)\n",
    "        \n",
    "        if shape[0] > 16:\n",
    "            sigma = float(shape[0]) / 16#16.0\n",
    "            kernel_size = (int(sigma) * 2 - 1)\n",
    "            blur = smoothing.GaussianSmoothing(1, kernel_size, sigma=sigma)\n",
    "            mask = blur(mask)\n",
    "        mask = mask[0, 0].to(device)\n",
    "#         # only do main layers 4 and up.\n",
    "#         if not layer.startswith('layer') or int(layer.split('.', 2)[0][5:]) < 4:\n",
    "#             mask.zero_()\n",
    "#         # and skip beyond layer 11.\n",
    "#         if layer.startswith('layer') and int(layer.split('.', 2)[0][5:]) > 11:\n",
    "#             mask.zero_()\n",
    "\n",
    "        #frac = -5\n",
    "        parent.adain = ApplyMaskedStyle(mask, frac)\n",
    "    return masked_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_model = load_seq_stylegan('bedroom', mconv='seq', truncation=0.90)\n",
    "nethook.set_requires_grad(False, gan_model)\n",
    "zds = zdataset.z_dataset_for_model(gan_model, size=1000)\n",
    "\n",
    "image_number=41\n",
    "z = zds[image_number][0][None].cuda()\n",
    "with torch.no_grad(): \n",
    "    original = gan_model(z)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PW = paintwidget.PaintWidget(image=renormalize.as_url(\n",
    "    gan_model(z)[0]))\n",
    "def do_reset():\n",
    "    PW.mask = ''\n",
    "RB = labwidget.Button('reset').on('click', do_reset)\n",
    "show([PW, RB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = PW.mask\n",
    "#initial_z = torch.randn(1, 512, device='cuda')\n",
    "masked_model = make_masked_stylegan(\n",
    "                gan_model, z, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    one_light = masked_model(z)[0]\n",
    "    original = gan_model(z)[0]\n",
    "show([(renormalize.as_image(one_light), renormalize.as_image(original))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rewrite_utils.segmenter import load_segmenter\n",
    "from importlib import reload\n",
    "from rewrite_utils import imgviz\n",
    "\n",
    "segmodel, seglabels = load_segmenter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    seg = segmodel.segment_batch(original.unsqueeze(0))\n",
    "\n",
    "reload(imgviz)\n",
    "iv = imgviz.ImageVisualizer(256)\n",
    "show([[iv.segmentation(seg[0][0])], iv.segment_key(seg[0,0], segmodel)])\n",
    "mask = (seg[0][0] == 21).float().cpu()\n",
    "#mask = renormalize.as_url(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 11\n",
    "blur_kernel = (1/2**(dim))*torch.ones(dim, dim)\n",
    "blur_kernel = blur_kernel[None, None]\n",
    "#blur_kernel = blur_kernel.repeat(1, 1, 1, 1).cuda()\n",
    "\n",
    "# blur_conv = F.conv2d(mask[None, None], blur_kernel, padding=5).squeeze()\n",
    "# print(blur_conv.shape)\n",
    "# blur_conv[blur_conv!=0] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_model = make_masked_stylegan(\n",
    "                gan_model, z, blur_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def get_max_lamp(seg): #gets x,y coordinates of largest lamp\n",
    "    \n",
    "    #show(renormalize.as_image(im[0]))\n",
    "\n",
    "    '''with torch.no_grad():\n",
    "        lamps = segmodel.predict_single_class(im, 21, downsample=4)'''\n",
    "    seg = seg.squeeze().int()\n",
    "    print(seg.shape)\n",
    "#     seg[seg!=21] = 0 #lamp index in segmentation = 21\n",
    "#     seg[seg==21] = 1\n",
    "    \n",
    "    #binary_lamps = lamps[1]*1\n",
    "    \n",
    "    binary_lamps = np.uint8(seg.cpu())\n",
    "    lamp_centroids = []\n",
    "    max_lamps = [] \n",
    " \n",
    "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(binary_lamps, connectivity=8)\n",
    "    sizes = stats[:, -1]\n",
    "    \n",
    "    #print(output)\n",
    "#     print(nb_components)\n",
    "#     print(np.unique(output))\n",
    "#     print(sizes)\n",
    "    \n",
    "    if len(np.unique(output))<=1: #no lamps detected\n",
    "        a = torch.Tensor(np.zeros([seg.shape[1], seg.shape[1]]))\n",
    "        return a\n",
    "\n",
    "        \n",
    "\n",
    "    max_label = 1\n",
    "    max_size = sizes[1]\n",
    "    for i in range(2, nb_components):\n",
    "        if sizes[i] > max_size:\n",
    "            max_label = i\n",
    "            max_size = sizes[i]\n",
    "\n",
    "    max_lamp = np.zeros(output.shape)\n",
    "    print('max label', max_label)\n",
    "    max_lamp[output == max_label] = 1\n",
    "    #max_lamp = Image.fromarray(max_lamp)\n",
    "    #max_lamp_layers = [max_lamp.resize([conv_dim, conv_dim], resample=Image.BILINEAR) for conv_dim in conv_dims] # TODO put assertion that it's still binary\n",
    "\n",
    "#     a = [ndimage.zoom(max_lamp, conv_dim/256) for conv_dim in conv_dims]\n",
    "#     if i==0: \n",
    "#         print(a)\n",
    "\n",
    "#     #a = np.array([np.array(max_lamp_layer) for max_lamp_layer in max_lamp_layers])\n",
    "#     #a = np.array(max_lamp_layers)\n",
    "#     #max_lamp_int = np.uint8(max_lamp)\n",
    "\n",
    "#     max_lamps.append(a)\n",
    "    \n",
    "    return torch.from_numpy(max_lamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numbers = range(25)#  [46] [77] [59] [95]\n",
    "def pilim(idata):\n",
    "    return renormalize.as_image(idata)\n",
    "show([\n",
    "    [pilim(gan_model(zds[i][0][None].cuda())[0]), i]\n",
    "    for i in image_numbers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pilim(idata):\n",
    "    return renormalize.as_image(idata)\n",
    "for i in range(41, 42): \n",
    "    z = zds[i][0][None].cuda()\n",
    "    original = gan_model(z)[0]\n",
    "    with torch.no_grad():\n",
    "        _, lamps = segmodel.predict_single_class(original[None], 21)\n",
    "        seg = 1*lamps\n",
    "        print(seg.shape)\n",
    "        mask = get_max_lamp(seg).float()\n",
    "        #print(mask)\n",
    "        blur_conv = F.conv2d(mask[None, None], blur_kernel, padding=5).squeeze()\n",
    "        blur_conv[blur_conv!=0] = 1\n",
    "        \n",
    "        frac = -10\n",
    "        masked_model = make_masked_stylegan(\n",
    "                gan_model, z, blur_conv, frac)\n",
    "        \n",
    "        edited = masked_model(z)[0]\n",
    "    #print(blur_conv)\n",
    "    #print(frac)\n",
    "    \n",
    "    show([(pilim(original), pilim(blur_conv[None].repeat(3, 1, 1)), pilim(edited))])\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    _, lamps = segmodel.predict_single_class(original[None], 21)\n",
    "    lamps = 1*lamps\n",
    "print(lamps.shape)\n",
    "print(torch.max(lamps))\n",
    "print(torch.min(lamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}